# -*- coding: utf-8 -*-
"""metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KsAiX-Dt53zKS8BTcr-L8S-lBKp9sJ8g

# **Imports**
"""
import os
import sys
sys.path.append("/content/")
sys.path.append("/content/lpips-tensorflow/")
import numpy as np
from tensorflow.keras import layers
import lpips_tf
import tensorflow_hub as tfh
import tensorflow as tf
import tensorflow_gan as tfg

"""# **Facilities Manger**"""

HR = 28
LR = 7
BUFFER_SIZE = 10000
BATCH_SIZE = 2000
noise_dim = 100
ensemble_size = 40
max_pixels = 255
THREADS_IN_PARALLEL = 1

"""# **Pathes**"""

# Commented out IPython magic to ensure Python compatibility.
# %mkdir /content/drive/My\ Drive/HUJI/Ami\ Wisel/CFID/MNIST/results
# %cd /content/drive/My\ Drive/HUJI/Ami\ Wisel/CFID/BACKUP/sensitive-MNIST-project/results
# %ls
DIR = '/content/drive/My Drive/HUJI/Ami Wisel/CFID/sensitive-MNIST-project/'
DIR_BACKUP = '/content/drive/My Drive/HUJI/Ami Wisel/CFID/BACKUP/sensitive-MNIST-project/'

# checkpoint_directory = DIR+"training_checkpoints"  # CHANGE to DRIVE .../Ami
checkpoint_directory = DIR_BACKUP+"training_checkpoints"  # CHANGE to DRIVE .../Ami
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt")

def downsample(H):
    L = tf.reshape(H,(-1, 7, 4, 7, 4))
    L = tf.reduce_mean(L,axis=4)
    L = tf.reduce_mean(L,axis=2)
    L = tf.cast(tf.reshape(L, (H.shape[0], LR, LR, H.shape[-1])), tf.float64)

    return L

def downsample_np(H):
    L = np.reshape(H,(-1, 7, 4, 7, 4))
    L = np.mean(L,axis=4)
    L = np.mean(L,axis=2)
    L = np.reshape(L, (H.shape[0], LR, LR, H.shape[-1])).astype(np.float64)

    return L

"""# **Classification models**"""

class MNIST(tf.keras.models.Model):
  """Model representing a MNIST classifier."""

  def __init__(self, output_activation="softmax"):
    super(MNIST, self).__init__()
    self.layer_1 = tf.keras.layers.Dense(64)
    self.layer_2 = tf.keras.layers.Dense(10, activation=output_activation)

  def call(self, inputs):
    casted = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(inputs)
    flatten = tf.keras.layers.Flatten()(casted)

    def normalize_fn(x):
      return x / tf.reduce_max(tf.gather(x, 0))

    normalize = tf.keras.layers.Lambda(normalize_fn)(flatten)
    x = self.layer_1(normalize)
    output = self.layer_2(x)
    return output

"""# **Metrics**"""

# utils
def pinv_diagonal_matrix(A):
    assert(len(A.shape)==1)
    too_large_mask = A>=1e+15   
    too_littlemask = A<=1e-15
    B = 1./A
    B = np.where(too_large_mask, 0, B)    
    B = np.where(too_littlemask, 1, B)  
    return B  

def nullify_small_values(A):
    too_littlemask = A<=1e-15
    A = np.where(too_littlemask, 0, A)  
    return A  
MNIST_calssifier = tf.keras.models.load_model(os.path.join(DIR_BACKUP, 'classification_model'))
MNIST_calssifier.compile()

def FID(y_true, y_predict, x_true=None, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]
    # print(y_predict.shape, y_true.shape)
    y_predict = np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,-1))
    y_true = np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,-1))

    # mean computations
    m_y_true = np.mean(y_true, axis=0)
    m_y_predict = np.mean(y_predict, axis=0)

    # diagonal covariances computations    
    с_y_true_y_true    =    np.mean(np.multiply(y_true   -m_y_true,      y_true   -m_y_true), axis=0)
    с_y_predict_y_predict = np.mean(np.multiply(y_predict-m_y_predict,   y_predict-m_y_predict), axis=0)

    # Gaussians distance
    m_dist = np.einsum('...k,...k->...',m_y_true-m_y_predict,m_y_true-m_y_predict)
    c_dist = np.sum(с_y_true_y_true + с_y_predict_y_predict - 2*(np.sqrt(np.multiply(с_y_true_y_true,с_y_predict_y_predict))),axis=0)
    return m_dist + c_dist

def CFID(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]

    y_predict = np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,-1))
    y_true = np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,-1))
    x_true = np.reshape(np.transpose(x_true, [0,3,1,2]), (k*n,-1))

    # mean computations
    m_y_true =    np.mean(y_true, axis=0)
    m_y_predict = np.mean(y_predict, axis=0)
    m_y_true    = np.mean(y_true, axis=0)
    m_y_predict = np.mean(y_predict, axis=0)
    m_x_true    = np.mean(x_true, axis=0)

    # diagonal covariances computations    
    с_y_predict_x_true    = nullify_small_values(np.mean(np.multiply(y_predict-m_y_predict,   x_true   -m_x_true),axis=0))
    с_y_true_x_true       = nullify_small_values(np.mean(np.multiply(y_true   -m_y_true,      x_true   -m_x_true),axis=0))
    с_y_predict_y_predict = nullify_small_values(np.mean(np.multiply(y_predict-m_y_predict,   y_predict-m_y_predict),axis=0))
    с_y_true_y_true       = nullify_small_values(np.mean(np.multiply(y_true   -m_y_true,      y_true   -m_y_true),axis=0))
    с_x_true_x_true       = nullify_small_values(np.mean(np.multiply(x_true   -m_x_true,      x_true   -m_x_true),axis=0))
    с_x_true_y_true       = nullify_small_values(np.mean(np.multiply(x_true   -m_x_true,      y_true   -m_y_true),axis=0))
    с_x_true_y_predict    = nullify_small_values(np.mean(np.multiply(x_true   -m_x_true,      y_predict-m_y_predict),axis=0))

    # conditoinal means and covariances
    inv_с_x_true_x_true = pinv_diagonal_matrix(с_x_true_x_true)
    A = np.multiply(inv_с_x_true_x_true, (x_true-m_x_true))
    m_y_true_given_x_true =    m_y_true    + np.multiply(с_y_true_x_true, A)
    m_y_predict_given_x_true = m_y_predict + np.multiply(с_y_predict_x_true, A)
    c_y_true_given_x_true    = nullify_small_values(с_y_true_y_true       - np.multiply(с_y_true_x_true,    np.multiply(inv_с_x_true_x_true, с_x_true_y_true)))
    c_y_predict_given_x_true = nullify_small_values(с_y_predict_y_predict - np.multiply(с_y_predict_x_true, np.multiply(inv_с_x_true_x_true, с_x_true_y_predict)))
    
    # Gaussians distance    
    m_dist = np.mean(np.einsum('...k,...k->...',m_y_true_given_x_true-m_y_predict_given_x_true,m_y_true_given_x_true-m_y_predict_given_x_true), axis=0)
    c_dist = np.sum(c_y_true_given_x_true + c_y_predict_given_x_true - 2*(np.sqrt(np.multiply(c_y_predict_given_x_true,c_y_true_given_x_true))),axis=0)
    # print('m_dist + c_dist',m_dist, c_dist)
    return m_dist + c_dist

def FID_tf(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]

    y_predict = np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,-1))
    y_true = np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,-1))
    return tfg.eval.diagonal_only_frechet_classifier_distance_from_activations(tf.convert_to_tensor(y_true),tf.convert_to_tensor(y_predict))

def PSNR(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]
    y_predict = (np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,HR,HR,1))*(max_pixels/2))+(max_pixels/2)
    y_true = (np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,HR,HR,1))*(max_pixels/2))+(max_pixels/2)
    y_true_RGB = tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_true))
    y_predict_RGB = tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_predict))

    val = tf.reduce_mean(tf.image.psnr(y_true_RGB,y_predict_RGB, max_val=max_pixels), axis=0)
    return 1/val

def SSIM(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]
    y_predict = (np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,HR,HR,1))*(max_pixels/2))+(max_pixels/2)
    y_true = (np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,HR,HR,1))*(max_pixels/2))+(max_pixels/2)
    y_true_RGB = tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_true))
    y_predict_RGB = tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_predict))

    val = tf.reduce_mean(tf.image.ssim(y_true_RGB,y_predict_RGB, max_val=max_pixels), axis=0)
    return 1/val

# def Sliced_Wasserstein(y_true, y_predict, x_true, labels=None):
#     print(y_true.shape, y_predict.shape)
#     n = y_true.shape[0]
#     k = y_predict.shape[-1]

#     y_predict = np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,HR,HR,1))
#     y_true =np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,HR,HR,1))
#     val = tfg.eval.sliced_wasserstein_distance(tf.image.resize(tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_true)), [32,32]),tf.image.resize(tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_predict)), [32,32]) , use_svd=True)
#     return val 

def Inception_Score(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]
    y_predict = tf.cast(((np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,HR,HR,1))*127.5)+127.5), tf.uint8)
    y_predict_IS = tfg.eval.classifier_score(y_predict,MNIST_calssifier)
    return 1/y_predict_IS


@tf.function
def __inner_LPIPS(y_true, y_predict, x_true, labels=None):
    n = y_true.shape[0]
    k = y_predict.shape[-1]
    y_predict = tf.reshape(tf.transpose(y_predict, [0,3,1,2]), (k*n,HR,HR,1))
    y_true = tf.reshape(tf.transpose(y_true, [0,3,1,2]), (k*n,HR,HR,1))
    y_true = tf.reshape(tf.transpose(y_true, [0,3,1,2]), (k*n,HR,HR,1))
    y_true_RGB = tf.cast(tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_true)), tf.float32)
    y_predict_RGB = tf.cast(tf.image.grayscale_to_rgb(tf.convert_to_tensor(y_predict)), tf.float32)
    y_true_RGB = tf.image.resize(y_true_RGB,[64,64])
    y_predict_RGB = tf.image.resize(y_predict_RGB,[64,64])
    
    val = lpips_tf.lpips(y_true_RGB, y_predict_RGB, model='net-lin', net='alex')
    val = tf.reduce_mean(val, axis=0)  
    return val
def LPIPS(y_true, y_predict, x_true, labels=None):

    return __inner_LPIPS(y_true, y_predict, x_true, labels).numpy()

# def __GMM_MC(y_true, y_predict, x_true, labels=None):
#     n = y_true.shape[0]
#     k = y_predict.shape[-1]
#     y_predict = np.reshape(np.transpose(y_predict, [0,3,1,2]), (k*n,-1))
#     y_true = np.reshape(np.transpose(y_true, [0,3,1,2]), (k*n,-1))

#     GMM1 = mixture.GaussianMixture(n_components=21, max_iter=200, n_init=3)
#     GMM2 = mixture.GaussianMixture(n_components=21, max_iter=200, n_init=3)
#     GMM1.fit_predict(y_true)
#     GMM2.fit_predict(y_predict)
    
#     def gmm_js(gmm_p, gmm_q, n_samples=10**5):
#         X = gmm_p.sample(n_samples)[0]
#         log_p_X = gmm_p.score_samples(X)
#         log_q_X = gmm_q.score_samples(X)
#         log_mix_X  = np.logaddexp(log_p_X, log_q_X)

#         Y = gmm_q.sample(n_samples)[0]
#         log_p_Y = gmm_p.score_samples(Y)
#         log_q_Y = gmm_q.score_samples(Y)
#         log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)

#         return (log_p_X.mean() - (log_mix_X.mean() - np.log(2))
#                 + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) / 2


#     val = gmm_js(GMM1, GMM2)
#     return val

def CAFID(y_true, y_predict, x_true=None, labels=None):
    classes = 10
    try:
        y_true = y_true.numpy()
        y_predict = y_predict.numpy()
    except:
        pass

    results = np.zeros(10)
    weights = np.ones(10)
    for cls in range(10):
        cls_mask = labels == cls
        cls_y_true = y_true[cls_mask]
        cls_y_predict = y_predict[cls_mask]
        cls_fid = FID(cls_y_true, cls_y_predict)
        results[cls] = cls_fid
        weights[cls] = sum(cls_mask)/len(cls_mask)
    return np.dot(results,weights)

# def COP(x,y):
#     p=1
#     k=1
#     n = x.shape[0]
#     x = tf.reshape(x[:,:,0], (-1,n))
#     y = tf.reshape(y[:,0,0], (-1,n))
#     A=cp.Variable((p,k))
#     b=cp.Variable((p,1))
#     C=cp.Variable((p,k))
#     d=cp.Variable((p,1))
#     u = cp.matmul(A,x)+cp.kron(np.ones((1,n)),b)
#     L = cp.matmul(C,x)+cp.kron(np.ones((1,n)),d)
#     obj1 = cp.sum((cp.multiply(L,y)-u)**2)
#     obj2 = sum([-2*cp.log(L[j,i]+1e-5) for j in range(p) for i in range(n)])
#     objective = cp.Minimize(obj1 + obj2)
#     constraints = [L>=0]
#     prob = cp.Problem(objective,constraints)
#     result = prob.solve(solver='SCS',verbose=False, max_iters=200)
#     return A.value, b.value, C.value, d.value

# def CFID3(y_true, y_predict, x_true=None, labels=None):
#     A_true, b_true, C_true, d_true = COP(x_true, y_true)    
#     A_predict, b_predict, C_predict, d_predict = COP(x_true, y_predict)
#     m_true          = np.matmul(A_true, x_true) + b_true
#     m_predict       = np.matmul(A_predict, x_true) + b_predict
#     с_y_true_y_true1       = np.matmul(C_true, x_true) + d_true
#     с_y_predict_y_predict1 = np.matmul(C_predict, x_true) + d_predict

#     m_dist = tf.reduce_mean(tf.einsum('...k,...k->...',m_true -m_predict,m_true-m_predict), axis=0)
#     c_dist = tf.reduce_mean(tf.einsum('tii->t',с_y_true_y_true1 + с_y_predict_y_predict1 - 2*(tf.sqrt(tf.einsum('tij,tjk->tik', с_y_true_y_true1,с_y_predict_y_predict1)))))

#     # print('CFID3 : ', (m_dist/100+c_dist/100))
#     return (m_dist + c_dist)

def LR_consistence(y_true, y_predict, x_true=None, labels=None):
    # t1 = time.time()
    # diff = downsample(y_predict)-downsample(y_true)
    diff = downsample_np(y_predict)-downsample_np(y_true)

    # return tf.reduce_mean(tf.math.multiply(diff, diff))    
    return np.mean(np.einsum('tijk,tijk->tk', diff, diff))